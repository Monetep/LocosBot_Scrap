{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivos de este NB:\n",
    "    -Obtener todos los links de la front page de Destructoid.com. COMPLETO\n",
    "    -Entrar a cada uno de los links, y obtener el titulo, y el texto de cada uno. COMPLETO\n",
    "    -Dejar en un formato tabular la informacion con las siguientes columnas: COMPLETO\n",
    "        -Pagina\n",
    "        -Titulo\n",
    "        -Subtitulo\n",
    "        -Parrafos_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [03:49<00:00,  9.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "site= \"https://www.destructoid.com/\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "final = pd.DataFrame()\n",
    "\n",
    "def scrapeo_dtoid(review):\n",
    "                \n",
    "    req = Request(review,headers=hdr)\n",
    "    page = urlopen(req)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    #Title\n",
    "    title = soup.find('title').get_text()    \n",
    "    #SubTitle\n",
    "    subtitle = soup.find(id=\"subtitle-large\")\n",
    "    subt = subtitle.get_text()    \n",
    "    #Parrafos\n",
    "    pars = soup.find_all('p')\n",
    "    parragraphs = [p.get_text() for p in pars]\n",
    "    \n",
    "    p_final = []\n",
    "    for i in range(0, len(parragraphs)):\n",
    "        if parragraphs[i] == '\\nYou are logged out. Login | Sign up\\xa0\\n\\xa0':\n",
    "            break\n",
    "        elif (\"https://t.co\" not in parragraphs[i]) and (parragraphs[i] != ''):\n",
    "            p_final.append(parragraphs[i])\n",
    "            \n",
    "    almacenado = pd.DataFrame({\n",
    "            \"site\": '',\n",
    "            \"title\": '',\n",
    "            \"subtitle\": '',\n",
    "            \"parrafo\": p_final\n",
    "        })   \n",
    "    almacenado['site'] = site\n",
    "    almacenado['title'] = title\n",
    "    almacenado['subtitle'] = subt\n",
    "\n",
    "    for i in range(0, len(almacenado)):\n",
    "        if almacenado['subtitle'][i] == almacenado['parrafo'][i]:\n",
    "            almacenado = almacenado.drop(almacenado.index[i])\n",
    "                    \n",
    "    return almacenado\n",
    "        \n",
    "lista_links_dtoid = links['links'].drop_duplicates().values.tolist()\n",
    "for i in tqdm(range(0, len(lista_links_dtoid))):    \n",
    "    final = final.append(scrapeo_dtoid(lista_links_dtoid[i]))\n",
    "    time.sleep(random.randint(2, 10))    \n",
    "    \n",
    "\n",
    "file_time = 'dtoid_scrap_{date:%Y-%m-%d_%H_%M_%S}.csv'.format(date=datetime.datetime.now())\n",
    "              \n",
    "final.to_csv(file_time, sep='|', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde acá comienza el nb para desarrollo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "site= \"https://www.destructoid.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = Request(site,headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "page.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparticle_title = soup.select(\"h2.sparticle_title a\")\n",
    "sparticle = []\n",
    "for i in range(0, len(sparticle_title)):\n",
    "    sparticle.append(sparticle_title[i].get('href'))\n",
    "\n",
    "links = pd.DataFrame({\n",
    "        \"links\": sparticle\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtoid_link = links['dtoid_link'] = links[\"links\"].str.contains(\"http\")\n",
    "links = links[~dtoid_link]\n",
    "links['links'] = str(site) + links['links'].astype(str)\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.destructoid.com/review-in-progress-world-of-warcraft-battle-for-azeroth-515131.phtml'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_links_dtoid = links['links'].drop_duplicates().values.tolist()\n",
    "#for i in range(0, len(lista_links_dtoid)):\n",
    "#    print(lista_links_dtoid[i])\n",
    "#    time.sleep(random.randint(1, 3))\n",
    "\n",
    "review = lista_links_dtoid[1]\n",
    "req = Request(review,headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title\n",
    "title = soup.find('title').get_text()\n",
    "#SubTitle\n",
    "subtitle = soup.find(id=\"subtitle-large\")\n",
    "subt = subtitle.get_text()\n",
    "#Parrafos\n",
    "pars = soup.find_all('p')\n",
    "\n",
    "#p2 = [p.extract() for p in pars]\n",
    "parragraphs = [p.get_text() for p in pars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final = []\n",
    "for i in range(0, len(parragraphs)):\n",
    "    if parragraphs[i] == '\\nYou are logged out. Login | Sign up\\xa0\\n\\xa0':\n",
    "        break\n",
    "    elif (\"https://t.co\" not in parragraphs[i]) and (parragraphs[i] != ''):\n",
    "        p_final.append(parragraphs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtoid_scrap_2018-08-19_19_53_17.csv\n"
     ]
    }
   ],
   "source": [
    "almacenado = pd.DataFrame({\n",
    "        \"site\": '',\n",
    "        \"title\": '',\n",
    "        \"subtitle\": '',\n",
    "        \"parrafo\": p_final\n",
    "    })   \n",
    "almacenado['site'] = site\n",
    "almacenado['title'] = title\n",
    "almacenado['subtitle'] = subt\n",
    "\n",
    "for i in range(0, len(almacenado)):\n",
    "    if almacenado['subtitle'][i] == almacenado['parrafo'][i]:\n",
    "        almacenado = almacenado.drop(almacenado.index[i])\n",
    "\n",
    "almacenado\n",
    "\n",
    "file_time = 'dtoid_scrap_{date:%Y-%m-%d_%H_%M_%S}.csv'.format(date=datetime.datetime.now())\n",
    "              \n",
    "almacenado.to_csv(file_time, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
